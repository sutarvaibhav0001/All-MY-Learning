{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a798a3e",
   "metadata": {},
   "source": [
    "Here are **detailed notes on Word2Vec (CBOW model)** ‚Äî covering theory, working, architecture, equations, and examples üëá\n",
    "\n",
    "---\n",
    "\n",
    "# **Word2Vec (CBOW Model) ‚Äì Full Notes with Example**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **1. Introduction to Word2Vec**\n",
    "\n",
    "**Word2Vec** is a **neural network-based model** used to learn **word embeddings** ‚Äî numerical vector representations of words capturing their meanings, similarities, and relationships.\n",
    "Developed by **Tomas Mikolov et al., 2013 (Google)**.\n",
    "\n",
    "It converts words into vectors such that:\n",
    "\n",
    "* Similar words have **similar vector representations**.\n",
    "* Words with similar context appear **close in the vector space**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© **2. Two Main Architectures of Word2Vec**\n",
    "\n",
    "1. **CBOW (Continuous Bag of Words)** ‚Äì predicts a **target word** from **context words**.\n",
    "2. **Skip-Gram** ‚Äì predicts **context words** from a **target word**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è **3. Working of CBOW Model**\n",
    "\n",
    "### üéØ **Goal:**\n",
    "\n",
    "Predict the **target word** based on the surrounding **context words**.\n",
    "\n",
    "Example:\n",
    "Sentence: `\"The cat sits on the mat\"`\n",
    "\n",
    "If we choose a **window size = 2**,\n",
    "for the target word `\"sits\"`:\n",
    "\n",
    "* **Context words** = [\"The\", \"cat\", \"on\", \"the\"]\n",
    "\n",
    "CBOW tries to predict `\"sits\"` from its context.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **4. CBOW Architecture**\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Input layer:** Takes context words.\n",
    "2. **Projection layer (Hidden layer):** Average their embeddings.\n",
    "3. **Output layer:** Predicts target word using softmax.\n",
    "\n",
    "### Diagram (Conceptual)\n",
    "\n",
    "```\n",
    "Context Words ‚Üí Embedding Lookup ‚Üí Average ‚Üí Hidden Layer ‚Üí Softmax ‚Üí Target Word\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìò **5. Mathematical Explanation**\n",
    "\n",
    "Let‚Äôs say:\n",
    "\n",
    "* Vocabulary size = `V`\n",
    "* Embedding dimension = `N`\n",
    "* Context size = `C` (number of context words)\n",
    "\n",
    "### 1Ô∏è‚É£ **Input:**\n",
    "\n",
    "Context words represented as **one-hot vectors** of size `V`.\n",
    "\n",
    "Example vocabulary = {the, cat, sits, on, mat}\n",
    "‚Üí V = 5\n",
    "\n",
    "Each word ‚Üí [0, 0, 1, 0, 0] (depending on position)\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Projection (Hidden) Layer:**\n",
    "\n",
    "Each one-hot vector is multiplied with **Weight matrix W** (V √ó N)\n",
    "to get the **word embedding**.\n",
    "\n",
    "For each context word ( w_i ):\n",
    "[\n",
    "h_i = W^T \\cdot x_i\n",
    "]\n",
    "\n",
    "Then take **average** of all context embeddings:\n",
    "[\n",
    "h = \\frac{1}{C} \\sum_{i=1}^{C} h_i\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **Output Layer:**\n",
    "\n",
    "We use another matrix ( W' ) (N √ó V) to map back to vocabulary space.\n",
    "\n",
    "[\n",
    "u = W'^T \\cdot h\n",
    "]\n",
    "\n",
    "Then apply **Softmax** to predict target word probability:\n",
    "\n",
    "[\n",
    "P(w_t | context) = \\frac{e^{u_{w_t}}}{\\sum_{j=1}^{V} e^{u_j}}\n",
    "]\n",
    "\n",
    "The model is trained using **cross-entropy loss**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è **6. Training Process**\n",
    "\n",
    "* Randomly initialize word vectors.\n",
    "* For each training example:\n",
    "\n",
    "  1. Take context words ‚Üí input\n",
    "  2. Predict target word ‚Üí output\n",
    "  3. Compute error (using softmax)\n",
    "  4. Backpropagate error\n",
    "  5. Update word vectors (in `W` and `W'`)\n",
    "\n",
    "After many iterations ‚Üí embeddings capture semantic meaning.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **7. Example**\n",
    "\n",
    "### Example Sentence:\n",
    "\n",
    "> ‚ÄúThe dog barks loudly‚Äù\n",
    "\n",
    "Vocabulary: [the, dog, barks, loudly]\n",
    "\n",
    "Let window size = 2\n",
    "\n",
    "| Target | Context words |\n",
    "| ------ | ------------- |\n",
    "| the    | [dog]         |\n",
    "| dog    | [the, barks]  |\n",
    "| barks  | [dog, loudly] |\n",
    "| loudly | [barks]       |\n",
    "\n",
    "So, training pairs:\n",
    "\n",
    "* Input: [the, barks] ‚Üí Output: dog\n",
    "* Input: [dog, loudly] ‚Üí Output: barks\n",
    "* Input: [barks] ‚Üí Output: loudly\n",
    "  and so on.\n",
    "\n",
    "After training, embeddings will learn:\n",
    "\n",
    "```\n",
    "dog ‚âà cat\n",
    "barks ‚âà meows\n",
    "the ‚âà a\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **8. Key Features of CBOW**\n",
    "\n",
    "| Feature  | Description           |\n",
    "| -------- | --------------------- |\n",
    "| Input    | Context words         |\n",
    "| Output   | Target word           |\n",
    "| Speed    | Faster than Skip-gram |\n",
    "| Best for | Large datasets        |\n",
    "| Captures | Frequent words better |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è **9. Comparison: CBOW vs Skip-Gram**\n",
    "\n",
    "| Feature    | CBOW                     | Skip-Gram                          |\n",
    "| ---------- | ------------------------ | ---------------------------------- |\n",
    "| Predicts   | Target word from context | Context words from target          |\n",
    "| Efficiency | Faster                   | Slower                             |\n",
    "| Handles    | Frequent words well      | Rare words better                  |\n",
    "| Output     | One word                 | Multiple context words             |\n",
    "| Use case   | Large corpus             | Small corpus or rare-word analysis |\n",
    "\n",
    "---\n",
    "\n",
    "## üß© **10. Advantages of CBOW**\n",
    "\n",
    "‚úÖ Simple and efficient to train\n",
    "‚úÖ Learns good embeddings for frequent words\n",
    "‚úÖ Captures semantic and syntactic word relationships\n",
    "‚úÖ Useful for large-scale NLP tasks\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è **11. Limitations**\n",
    "\n",
    "‚ùå Struggles with rare words\n",
    "‚ùå Ignores word order within context (bag-of-words assumption)\n",
    "‚ùå Static embeddings (same vector for all contexts)\n",
    "\n",
    "---\n",
    "\n",
    "## üí° **12. Practical Example (Using Python and Gensim)**\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"The cat sits on the mat. The dog lies on the rug.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Train CBOW model (sg=0 means CBOW)\n",
    "model = Word2Vec([tokens], vector_size=50, window=2, min_count=1, sg=0)\n",
    "\n",
    "# Show embedding for a word\n",
    "print(model.wv['cat'])\n",
    "\n",
    "# Find similar words\n",
    "print(model.wv.most_similar('cat'))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß© **13. Applications of Word2Vec (CBOW)**\n",
    "\n",
    "* Text classification\n",
    "* Sentiment analysis\n",
    "* Machine translation\n",
    "* Document similarity\n",
    "* Information retrieval\n",
    "* Recommendation systems\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ **14. Visualization Idea**\n",
    "\n",
    "If embeddings are plotted in 2D (after dimensionality reduction):\n",
    "\n",
    "```\n",
    "king - man + woman ‚âà queen\n",
    "Paris - France + Italy ‚âà Rome\n",
    "```\n",
    "\n",
    "This shows **semantic relationships** captured by Word2Vec embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In summary:**\n",
    "\n",
    "| Aspect         | CBOW Model Summary                            |\n",
    "| -------------- | --------------------------------------------- |\n",
    "| Input          | Context words                                 |\n",
    "| Output         | Target word                                   |\n",
    "| Type           | Predictive embedding model                    |\n",
    "| Learning       | Neural network                                |\n",
    "| Output vectors | Dense semantic word embeddings                |\n",
    "| Example        | Predict ‚Äúsits‚Äù from ‚Äúthe‚Äù, ‚Äúcat‚Äù, ‚Äúon‚Äù, ‚Äúthe‚Äù |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to **convert this into a well-formatted PDF with diagram and visual example (2D embedding plot)** for offline study?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e7047",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
