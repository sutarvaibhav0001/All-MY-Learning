{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aacb1c9",
   "metadata": {},
   "source": [
    "Here are **complete notes on Word Embeddings** ‚Äî covering concepts, types, advantages, disadvantages, and examples üëá\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Word Embeddings ‚Äî Full Notes**\n",
    "\n",
    "### üìò **Definition**\n",
    "\n",
    "**Word Embedding** is a technique in **Natural Language Processing (NLP)** to represent words as **dense vectors of real numbers** in a continuous vector space.\n",
    "It captures the **semantic meaning**, **relationships**, and **context** of words better than traditional methods like One-Hot Encoding or Bag of Words.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° **Intuition**\n",
    "\n",
    "Words with **similar meanings** have **similar vector representations**.\n",
    "Example:\n",
    "\n",
    "> ‚ÄúKing‚Äù ‚Äì ‚ÄúMan‚Äù + ‚ÄúWoman‚Äù ‚âà ‚ÄúQueen‚Äù\n",
    "\n",
    "So, embeddings help machines **understand semantic relationships** among words.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **How It Works**\n",
    "\n",
    "* Each word is represented as a **vector** (say, of 100 or 300 dimensions).\n",
    "* These vectors are **learned from large text corpora** using models like **Word2Vec**, **GloVe**, or **FastText**.\n",
    "* The position of a word vector in the space is such that **similar words are close** to each other (based on cosine similarity).\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Types of Word Embeddings**\n",
    "\n",
    "#### 1. **Word2Vec (by Google, 2013)**\n",
    "\n",
    "It learns embeddings using a neural network model with two main architectures:\n",
    "\n",
    "##### a) **CBOW (Continuous Bag of Words)**\n",
    "\n",
    "* Predicts a **word** given its **context** (surrounding words).\n",
    "* Example:\n",
    "  Context: ‚ÄúI ___ NLP‚Äù ‚Üí Predict: ‚Äúlove‚Äù\n",
    "\n",
    "##### b) **Skip-Gram**\n",
    "\n",
    "* Predicts **context words** from a **target word**.\n",
    "* Example:\n",
    "  Target: ‚Äúlove‚Äù ‚Üí Predict: ‚ÄúI‚Äù, ‚ÄúNLP‚Äù\n",
    "\n",
    "üìä **Use Case:** Works well for large datasets, captures semantic and syntactic meaning.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **GloVe (Global Vectors for Word Representation, by Stanford)**\n",
    "\n",
    "* Based on **word co-occurrence statistics** from a corpus.\n",
    "* Focuses on **global** statistical information (unlike Word2Vec which is local).\n",
    "\n",
    "üìä **Use Case:** Captures global relationships; good for semantic analogy tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **FastText (by Facebook)**\n",
    "\n",
    "* Represents each word as a **bag of character n-grams**.\n",
    "* Useful for **morphologically rich languages** or **rare/out-of-vocabulary (OOV)** words.\n",
    "\n",
    "üìä **Use Case:** Can infer embeddings for unseen words by using subword information.\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ **Example: Word2Vec using Python**\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    ['I', 'love', 'natural', 'language', 'processing'],\n",
    "    ['I', 'love', 'machine', 'learning'],\n",
    "    ['natural', 'language', 'processing', 'is', 'fun']\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=1)\n",
    "\n",
    "# Get vector of a word\n",
    "print(model.wv['language'])\n",
    "\n",
    "# Find similar words\n",
    "print(model.wv.most_similar('language'))\n",
    "```\n",
    "\n",
    "**Output Example:**\n",
    "\n",
    "```\n",
    "[array([...])]  # 50-dimension vector\n",
    "[('processing', 0.93), ('natural', 0.89), ('learning', 0.78)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Applications**\n",
    "\n",
    "* Sentiment analysis\n",
    "* Machine translation\n",
    "* Text classification\n",
    "* Question answering\n",
    "* Document similarity\n",
    "* Chatbots and recommendation systems\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Advantages**\n",
    "\n",
    "| Advantage                  | Description                                                                        |\n",
    "| -------------------------- | ---------------------------------------------------------------------------------- |\n",
    "| **Semantic understanding** | Captures relationships between words (e.g., \"Paris\" ‚Äì \"France\" + \"Italy\" ‚âà \"Rome\") |\n",
    "| **Dense representation**   | Low-dimensional and efficient compared to sparse BoW/One-Hot                       |\n",
    "| **Generalization**         | Handles similar words better, improving NLP model performance                      |\n",
    "| **Contextual similarity**  | Identifies similar meanings (e.g., \"doctor\" and \"physician\")                       |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå **Disadvantages**\n",
    "\n",
    "| Disadvantage                | Description                                                                                   |\n",
    "| --------------------------- | --------------------------------------------------------------------------------------------- |\n",
    "| **Static embeddings**       | Same vector for a word regardless of context (e.g., \"bank\" in \"river bank\" vs \"bank account\") |\n",
    "| **Requires large data**     | Needs huge corpus for meaningful vectors                                                      |\n",
    "| **Cannot handle OOV words** | Word2Vec & GloVe fail for unseen words (solved in FastText)                                   |\n",
    "| **Bias in data**            | May capture gender, race, or cultural biases from the training text                           |\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ **Visual Representation**\n",
    "\n",
    "Imagine a 3D vector space:\n",
    "\n",
    "```\n",
    "|                   * king\n",
    "|                * queen\n",
    "|          * man\n",
    "|       * woman\n",
    "|\n",
    "+--------------------------------\n",
    "```\n",
    "\n",
    "Words like *king* and *queen* are close, as are *man* and *woman*.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è **Comparison Table**\n",
    "\n",
    "| Feature                | Word2Vec                       | GloVe                       | FastText                    |\n",
    "| ---------------------- | ------------------------------ | --------------------------- | --------------------------- |\n",
    "| **Based on**           | Local context (neural network) | Global co-occurrence matrix | Subword (character n-grams) |\n",
    "| **Handles OOV words**  | ‚ùå                              | ‚ùå                           | ‚úÖ                           |\n",
    "| **Captures semantics** | ‚úÖ                              | ‚úÖ                           | ‚úÖ                           |\n",
    "| **Training speed**     | Fast                           | Moderate                    | Slower                      |\n",
    "| **Contextual info**    | Limited                        | Limited                     | Limited                     |\n",
    "| **Model type**         | Predictive                     | Count-based                 | Predictive + Subword        |\n",
    "\n",
    "---\n",
    "\n",
    "### üß± **Beyond Traditional Embeddings**\n",
    "\n",
    "Later models improved context-awareness:\n",
    "\n",
    "* **ELMo** ‚Äì Contextual embeddings using Bi-LSTM\n",
    "* **BERT** ‚Äì Transformer-based model giving context-dependent vectors\n",
    "* **GPT, RoBERTa, etc.** ‚Äì Advanced contextual embeddings for deep NLP tasks\n",
    "\n",
    "---\n",
    "\n",
    "### üîç **Quick Summary**\n",
    "\n",
    "| Aspect             | Traditional (BoW, One-Hot) | Word Embeddings |\n",
    "| ------------------ | -------------------------- | --------------- |\n",
    "| Representation     | Sparse                     | Dense           |\n",
    "| Meaning captured   | No                         | Yes             |\n",
    "| Dimensionality     | High                       | Low             |\n",
    "| Handles similarity | No                         | Yes             |\n",
    "\n",
    "---\n",
    "\n",
    "### üß∞ **Example Analogy**\n",
    "\n",
    "```python\n",
    "model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "```\n",
    "\n",
    "Output ‚Üí `[('queen', 0.89)]`\n",
    "\n",
    "This shows that embeddings capture **semantic relationships** like gender, roles, and country-capital analogies.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to generate a **PDF version** of these ‚ÄúWord Embedding Full Notes (with examples and visuals)‚Äù for download?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27596ea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
