{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a01fe19",
   "metadata": {},
   "source": [
    "Here‚Äôs a **complete and detailed note on Tokenization** ‚Äî perfect for your NLP or machine learning study materials.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† **Tokenization ‚Äì Full Notes**\n",
    "\n",
    "## üìò **1. Introduction**\n",
    "\n",
    "**Tokenization** is the process of splitting text into smaller units called **tokens**.\n",
    "Tokens can be **words, characters, or subwords**, depending on the level of tokenization.\n",
    "\n",
    "üëâ It‚Äôs the **first step in Natural Language Processing (NLP)** and text preprocessing.\n",
    "It helps computers understand and process text by breaking it into manageable pieces.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è **2. Why Tokenization is Important**\n",
    "\n",
    "| Purpose              | Description                                      |\n",
    "| -------------------- | ------------------------------------------------ |\n",
    "| üß© Simplification    | Converts complex text into structured units.     |\n",
    "| üìä Analysis          | Helps in building word frequency distributions.  |\n",
    "| üß† Input Preparation | Converts text to a form usable by ML/NLP models. |\n",
    "| üßπ Cleaning          | Removes punctuation and irrelevant symbols.      |\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ **3. Types of Tokenization**\n",
    "\n",
    "### (a) **Word Tokenization**\n",
    "\n",
    "Splitting text into individual words.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenization is the first step in NLP!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', '!']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### (b) **Sentence Tokenization**\n",
    "\n",
    "Splitting text into sentences.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello there! How are you doing today? Let's learn NLP.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "['Hello there!', 'How are you doing today?', \"Let's learn NLP.\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### (c) **Character Tokenization**\n",
    "\n",
    "Splitting text into individual characters.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "text = \"ChatGPT\"\n",
    "tokens = list(text)\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "['C', 'h', 'a', 't', 'G', 'P', 'T']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### (d) **Subword Tokenization**\n",
    "\n",
    "Breaks words into smaller meaningful parts, used in modern NLP models like **BERT** or **GPT**.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "unbelievable ‚Üí un + believe + able\n",
    "```\n",
    "\n",
    "This helps handle rare words and reduces vocabulary size.\n",
    "\n",
    "---\n",
    "\n",
    "### (e) **Regex Tokenization**\n",
    "\n",
    "Uses **regular expressions** to define token patterns.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(\"I'm learning NLP with ChatGPT.\")\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "['I', 'm', 'learning', 'NLP', 'with', 'ChatGPT']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß© **4. Tokenization Using Different Libraries**\n",
    "\n",
    "| Library          | Function                          | Example                  |\n",
    "| ---------------- | --------------------------------- | ------------------------ |\n",
    "| **NLTK**         | `word_tokenize`, `sent_tokenize`  | Traditional NLP tasks    |\n",
    "| **spaCy**        | `nlp(text)` then `token.text`     | Faster and more accurate |\n",
    "| **Hugging Face** | `AutoTokenizer.from_pretrained()` | For transformer models   |\n",
    "\n",
    "**Example (spaCy):**\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Tokenization simplifies NLP tasks.\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìà **5. Applications of Tokenization**\n",
    "\n",
    "* Preprocessing in **Sentiment Analysis**\n",
    "* Text classification\n",
    "* Named Entity Recognition (NER)\n",
    "* Machine Translation\n",
    "* Search engines (indexing words)\n",
    "* Language Modeling (GPT, BERT)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è **6. Advantages and Disadvantages**\n",
    "\n",
    "| Advantages                         | Disadvantages                       |\n",
    "| ---------------------------------- | ----------------------------------- |\n",
    "| Simplifies text for NLP models     | May lose meaning in context         |\n",
    "| Enables better vectorization       | Hard to handle abbreviations/slangs |\n",
    "| Reduces complexity of raw text     | Language-dependent rules            |\n",
    "| Essential for embedding generation | Requires cleaning & normalization   |\n",
    "\n",
    "---\n",
    "\n",
    "## üé® **7. Visual Representation**\n",
    "\n",
    "```\n",
    "Raw Text:  \"Tokenization is essential for NLP.\"\n",
    "\n",
    "          ‚Üì\n",
    "\n",
    "Word Tokenization ‚Üí ['Tokenization', 'is', 'essential', 'for', 'NLP', '.']\n",
    "\n",
    "Sentence Tokenization ‚Üí ['Tokenization is essential for NLP.']\n",
    "\n",
    "Character Tokenization ‚Üí ['T','o','k','e','n','i','z','a','t','i','o','n',' ',...]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **8. Tokenization Challenges**\n",
    "\n",
    "* **Ambiguity:** ‚ÄúU.S.A.‚Äù ‚Üí should it be one token or three?\n",
    "* **Compound words:** ‚ÄúNew York‚Äù vs ‚ÄúNew‚Äù and ‚ÄúYork‚Äù\n",
    "* **Languages without spaces:** Chinese, Japanese require special tokenizers\n",
    "* **Emojis and hashtags:** Need custom handling for social media text\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ **9. Tools for Tokenization**\n",
    "\n",
    "| Tool                           | Description                     |\n",
    "| ------------------------------ | ------------------------------- |\n",
    "| **NLTK**                       | Classical NLP library           |\n",
    "| **spaCy**                      | Industrial-strength NLP         |\n",
    "| **Transformers (HuggingFace)** | Modern model-based tokenization |\n",
    "| **TextBlob**                   | Simple text processing          |\n",
    "| **Moses Tokenizer**            | For machine translation         |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **10. Summary Table**\n",
    "\n",
    "| Type      | Description              | Example Output                   |\n",
    "| --------- | ------------------------ | -------------------------------- |\n",
    "| Word      | Splits text by words     | `['Hello', 'world', '!']`        |\n",
    "| Sentence  | Splits text by sentences | `['Hello world!']`               |\n",
    "| Character | Splits by each letter    | `['H','e','l','l','o']`          |\n",
    "| Subword   | Splits words into roots  | `['un', 'believe', 'able']`      |\n",
    "| Regex     | Uses pattern rules       | `['learning', 'NLP', 'ChatGPT']` |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to **generate a PDF** of this note (with formatted text, tables, and diagrams) for download and study use?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
