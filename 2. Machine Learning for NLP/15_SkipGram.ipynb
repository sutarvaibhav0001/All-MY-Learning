{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6f1027",
   "metadata": {},
   "source": [
    "Here are **full, clear, and exam-ready notes** on **Word2Vec Skip-Gram Model** â€” with **concepts, math, diagrams (explained textually)**, and **examples** ğŸ‘‡\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  **Word2Vec â€” Skip-Gram Model (Full Notes)**\n",
    "\n",
    "### ğŸ”¹ 1. Introduction\n",
    "\n",
    "**Word2Vec** is a technique in **Natural Language Processing (NLP)** to represent words as numerical vectors (called *word embeddings*), capturing **semantic relationships** between words.\n",
    "\n",
    "It was introduced by **Tomas Mikolov et al. (Google, 2013)**.\n",
    "\n",
    "Word2Vec has two main architectures:\n",
    "\n",
    "1. **CBOW (Continuous Bag of Words)**\n",
    "   â†’ Predicts *target word* from *context words*.\n",
    "2. **Skip-Gram**\n",
    "   â†’ Predicts *context words* from a *target word*.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 2. What is Skip-Gram?\n",
    "\n",
    "The **Skip-Gram model** works **opposite to CBOW**.\n",
    "Given a *center word*, it tries to **predict the surrounding context words**.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Sentence:\n",
    "\n",
    "> â€œI love natural language processingâ€\n",
    "\n",
    "If the **window size = 2**, then for each target word:\n",
    "\n",
    "| Target (Input Word) | Context Words (Output) |\n",
    "| ------------------- | ---------------------- |\n",
    "| I                   | love                   |\n",
    "| love                | I, natural             |\n",
    "| natural             | love, language         |\n",
    "| language            | natural, processing    |\n",
    "| processing          | language               |\n",
    "\n",
    "Here, **â€œloveâ€** is the *center word*, and **â€œIâ€** and **â€œnaturalâ€** are its *context words*.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 3. Objective of Skip-Gram\n",
    "\n",
    "Skip-Gram tries to **maximize the probability** of predicting context words given a target word.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "[\n",
    "J = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log P(w_{t+j} | w_t)\n",
    "]\n",
    "\n",
    "Where:\n",
    "\n",
    "* ( T ) â†’ total number of words in corpus\n",
    "* ( c ) â†’ context window size\n",
    "* ( w_t ) â†’ target word at position *t*\n",
    "* ( w_{t+j} ) â†’ context word\n",
    "\n",
    "The goal:\n",
    "ğŸ‘‰ Find word vectors that make nearby words have high probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 4. How Skip-Gram Works (Step-by-Step)\n",
    "\n",
    "### Step 1: Input Word\n",
    "\n",
    "Choose one word (the target word) from the text.\n",
    "\n",
    "### Step 2: Generate Training Pairs\n",
    "\n",
    "For each target word, pair it with its context words (within the window size).\n",
    "\n",
    "Example sentence: â€œThe dog barked loudlyâ€\n",
    "If window size = 2:\n",
    "\n",
    "| Input  | Output      |\n",
    "| ------ | ----------- |\n",
    "| dog    | The, barked |\n",
    "| barked | dog, loudly |\n",
    "\n",
    "### Step 3: Neural Network Architecture\n",
    "\n",
    "The model is a **simple 3-layer neural network**:\n",
    "\n",
    "1. **Input layer:** One-hot vector of target word\n",
    "2. **Hidden layer:** Dense representation (word embedding)\n",
    "3. **Output layer:** Softmax layer over all words in vocabulary\n",
    "\n",
    "#### Diagram (conceptual)\n",
    "\n",
    "```\n",
    "Input word â†’ Hidden Layer â†’ Output Layer\n",
    "   (one-hot)     (embedding)    (softmax probabilities)\n",
    "```\n",
    "\n",
    "### Step 4: Training\n",
    "\n",
    "* The model adjusts word vectors to increase the probability of correct context words.\n",
    "* **Loss function:** Cross-entropy between predicted and actual context.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 5. Probability Computation\n",
    "\n",
    "For each target-context pair:\n",
    "\n",
    "[\n",
    "P(w_O | w_I) = \\frac{\\exp(v'*{w_O} \\cdot v*{w_I})}{\\sum_{w=1}^{V} \\exp(v'*w \\cdot v*{w_I})}\n",
    "]\n",
    "\n",
    "Where:\n",
    "\n",
    "* ( v_{w_I} ): input (target) word vector\n",
    "* ( v'_{w_O} ): output (context) word vector\n",
    "* ( V ): size of vocabulary\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 6. Optimization Techniques\n",
    "\n",
    "To make training efficient for large vocabularies, two techniques are used:\n",
    "\n",
    "1. **Negative Sampling:**\n",
    "   Instead of updating weights for all words, update only a few â€œnegativeâ€ (random) words along with positive examples.\n",
    "\n",
    "2. **Hierarchical Softmax:**\n",
    "   Uses a binary tree structure to reduce computation cost of softmax.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 7. Example (Simplified)\n",
    "\n",
    "Letâ€™s take a small corpus:\n",
    "\n",
    "> â€œI love dogsâ€\n",
    "\n",
    "Vocabulary: [I, love, dogs]\n",
    "Window size = 1\n",
    "\n",
    "### Training pairs:\n",
    "\n",
    "| Input (Target) | Output (Context) |\n",
    "| -------------- | ---------------- |\n",
    "| I              | love             |\n",
    "| love           | I, dogs          |\n",
    "| dogs           | love             |\n",
    "\n",
    "During training:\n",
    "\n",
    "* â€œloveâ€ vector gets close to both â€œIâ€ and â€œdogsâ€\n",
    "* â€œIâ€ and â€œdogsâ€ get indirectly connected through â€œloveâ€\n",
    "\n",
    "ğŸ‘‰ After training, the **word embeddings** show **semantic similarity**:\n",
    "\n",
    "```\n",
    "cosine_similarity(love, dogs) â‰ˆ cosine_similarity(love, I)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 8. Skip-Gram vs CBOW (Comparison Table)\n",
    "\n",
    "| Feature            | Skip-Gram                 | CBOW                      |\n",
    "| ------------------ | ------------------------- | ------------------------- |\n",
    "| Input              | Target word               | Context words             |\n",
    "| Output             | Context words             | Target word               |\n",
    "| Training data size | Large                     | Smaller                   |\n",
    "| Works well for     | Rare words                | Frequent words            |\n",
    "| Training speed     | Slower                    | Faster                    |\n",
    "| Example            | â€œloveâ€ â†’ (â€œIâ€, â€œnaturalâ€) | (â€œIâ€, â€œnaturalâ€) â†’ â€œloveâ€ |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 9. Advantages\n",
    "\n",
    "âœ… Captures **semantic** and **syntactic** relationships\n",
    "âœ… Performs well on **rare words**\n",
    "âœ… Produces high-quality word embeddings\n",
    "âœ… Can use **negative sampling** for faster training\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 10. Disadvantages\n",
    "\n",
    "âŒ Requires large corpus to perform well\n",
    "âŒ Computationally expensive for large vocabularies\n",
    "âŒ Meaning depends on window size (context scope)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 11. Example in Code (Python using Gensim)\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [[\"i\", \"love\", \"natural\", \"language\", \"processing\"],\n",
    "             [\"word2vec\", \"is\", \"a\", \"powerful\", \"embedding\", \"model\"]]\n",
    "\n",
    "# Train Skip-Gram model (sg=1)\n",
    "model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, sg=1)\n",
    "\n",
    "# Get vector of a word\n",
    "print(model.wv['love'])\n",
    "\n",
    "# Find most similar words\n",
    "print(model.wv.most_similar('love'))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 12. Visualization (Explained Textually)\n",
    "\n",
    "Imagine a 2D plot of embeddings:\n",
    "\n",
    "```\n",
    "king â†’ (0.8, 0.6)\n",
    "queen â†’ (0.79, 0.58)\n",
    "man â†’ (0.72, 0.50)\n",
    "woman â†’ (0.70, 0.48)\n",
    "```\n",
    "\n",
    "Relationships like:\n",
    "\n",
    "```\n",
    "king - man + woman â‰ˆ queen\n",
    "```\n",
    "\n",
    "show how Skip-Gram embeddings capture *semantic meaning*.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ **Summary**\n",
    "\n",
    "| Concept      | Description                             |\n",
    "| ------------ | --------------------------------------- |\n",
    "| Model Type   | Predicts context from target word       |\n",
    "| Input        | Single word                             |\n",
    "| Output       | Context words                           |\n",
    "| Ideal For    | Rare words                              |\n",
    "| Optimization | Negative sampling, hierarchical softmax |\n",
    "| Output       | Dense vector representation of words    |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to generate a **PDF version of these full notes with diagrams and examples** (for study or printing)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca4215",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
