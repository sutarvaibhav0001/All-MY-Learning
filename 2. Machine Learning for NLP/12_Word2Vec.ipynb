{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0393bc8d",
   "metadata": {},
   "source": [
    "Here are **complete, easy-to-understand notes on Word2Vec** â€” including concepts, working, architectures, and examples.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  **Word2Vec â€“ Full Notes with Examples**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ **1. Introduction**\n",
    "\n",
    "**Word2Vec** is a **word embedding technique** developed by **Tomas Mikolov and team at Google (2013)**.\n",
    "It represents **words as dense vectors** (numerical form) such that **similar words are close together** in the vector space.\n",
    "\n",
    "ğŸ§© Example:\n",
    "\n",
    "* â€œKingâ€ â€“ [0.21, 0.98, -0.55, 0.33, â€¦]\n",
    "* â€œQueenâ€ â€“ [0.19, 0.89, -0.49, 0.29, â€¦]\n",
    "* â€œAppleâ€ â€“ [0.75, 0.12, 0.56, 0.02, â€¦]\n",
    "\n",
    "ğŸ“These numbers donâ€™t have direct meaning individually, but together they capture **semantic relationships** between words.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ **2. Why Word2Vec?**\n",
    "\n",
    "Before Word2Vec, methods like **One-Hot Encoding** or **Bag of Words (BoW)** had limitations:\n",
    "\n",
    "| Method           | Limitation                                           |\n",
    "| ---------------- | ---------------------------------------------------- |\n",
    "| One-Hot Encoding | Large, sparse vectors; no relationship between words |\n",
    "| Bag of Words     | Ignores context; cannot capture meaning or order     |\n",
    "| TF-IDF           | Focuses on word frequency, not semantic meaning      |\n",
    "\n",
    "âœ… **Word2Vec overcomes these** by:\n",
    "\n",
    "* Producing **dense and low-dimensional** vectors\n",
    "* Capturing **semantic similarity**\n",
    "* Preserving **contextual meaning**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ **3. Core Idea**\n",
    "\n",
    "Word2Vec is based on the **distributional hypothesis**:\n",
    "\n",
    "> â€œWords that appear in similar contexts have similar meanings.â€\n",
    "\n",
    "For example,\n",
    "â€œThe cat sat on the mat.â€\n",
    "â€œThe dog sat on the rug.â€\n",
    "\n",
    "â¡ï¸ â€œCatâ€ and â€œDogâ€ appear in similar contexts â†’ similar embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ **4. Word2Vec Architectures**\n",
    "\n",
    "There are **two main architectures**:\n",
    "\n",
    "### ğŸ§© (A) Continuous Bag of Words (CBOW)\n",
    "\n",
    "* Predicts **target word** based on **context words**.\n",
    "* Example:\n",
    "  Input: [â€œtheâ€, â€œcatâ€, â€œonâ€, â€œtheâ€, â€œmatâ€]\n",
    "  Target: â€œsatâ€\n",
    "  Model learns: context â†’ word\n",
    "\n",
    "ğŸ§  It averages the context embeddings and predicts the middle word.\n",
    "\n",
    "### ğŸ§© (B) Skip-Gram\n",
    "\n",
    "* Predicts **context words** based on **target word**.\n",
    "* Example:\n",
    "  Input: â€œsatâ€\n",
    "  Output: [â€œtheâ€, â€œcatâ€, â€œonâ€, â€œtheâ€, â€œmatâ€]\n",
    "\n",
    "ğŸ§  Works well for small datasets and captures **rare word relationships** better.\n",
    "\n",
    "| Feature        | CBOW                | Skip-Gram           |\n",
    "| -------------- | ------------------- | ------------------- |\n",
    "| Predicts       | Target from context | Context from target |\n",
    "| Training speed | Faster              | Slower              |\n",
    "| Good for       | Frequent words      | Rare words          |\n",
    "| Accuracy       | Lower               | Higher              |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ **5. Training Objective**\n",
    "\n",
    "### â¤ CBOW Objective\n",
    "\n",
    "Maximize probability of target word given context:\n",
    "[\n",
    "P(w_t | w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2})\n",
    "]\n",
    "\n",
    "### â¤ Skip-Gram Objective\n",
    "\n",
    "Maximize probability of context words given target:\n",
    "[\n",
    "P(w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2} | w_t)\n",
    "]\n",
    "\n",
    "Both use a **neural network** to learn weights â†’ word vectors.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ **6. Word2Vec Workflow**\n",
    "\n",
    "1. **Collect text data**\n",
    "2. **Build vocabulary**\n",
    "3. **Define window size** (number of context words)\n",
    "4. **Choose architecture (CBOW or Skip-Gram)**\n",
    "5. **Train model**\n",
    "6. **Obtain embeddings**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ **7. Example Using Gensim in Python**\n",
    "\n",
    "```python\n",
    "# Example: Word2Vec using Gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    [\"i\", \"love\", \"machine\", \"learning\"],\n",
    "    [\"word2vec\", \"is\", \"a\", \"powerful\", \"technique\"],\n",
    "    [\"i\", \"love\", \"natural\", \"language\", \"processing\"]\n",
    "]\n",
    "\n",
    "# Create Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, sg=1)\n",
    "\n",
    "# Save vectors\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Get vector for a word\n",
    "print(model.wv[\"love\"])\n",
    "\n",
    "# Find most similar words\n",
    "print(model.wv.most_similar(\"love\"))\n",
    "```\n",
    "\n",
    "**Output (Example):**\n",
    "\n",
    "```\n",
    "Vector for 'love': [ 0.123 -0.245 ... 0.056]\n",
    "Most similar to 'love': [('like', 0.89), ('enjoy', 0.85), ('adore', 0.82)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ **8. Real-World Example (Analogies)**\n",
    "\n",
    "Word2Vec captures **semantic relationships**:\n",
    "[\n",
    "\\text{King} - \\text{Man} + \\text{Woman} â‰ˆ \\text{Queen}\n",
    "]\n",
    "\n",
    "Example in Python:\n",
    "\n",
    "```python\n",
    "print(model.wv.most_similar(positive=['king', 'woman'], negative=['man']))\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "[('queen', 0.94), ('princess', 0.88)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ **9. Advantages**\n",
    "\n",
    "âœ… Captures **semantic and syntactic meaning**\n",
    "âœ… Produces **dense**, **low-dimensional** vectors\n",
    "âœ… Works well for **large text corpora**\n",
    "âœ… Supports **similarity and analogy** operations\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ **10. Limitations**\n",
    "\n",
    "âŒ Cannot handle **out-of-vocabulary (OOV)** words\n",
    "âŒ Does not consider **word order beyond fixed window**\n",
    "âŒ Fails to capture **polysemy** (multiple meanings of a word)\n",
    "âŒ Requires large training data for good results\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ **11. Applications**\n",
    "\n",
    "ğŸ“ Used in:\n",
    "\n",
    "* Sentiment Analysis\n",
    "* Machine Translation\n",
    "* Document Similarity\n",
    "* Chatbots and Q&A systems\n",
    "* Recommendation Systems\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ **12. Visualization Example**\n",
    "\n",
    "Imagine a 2D projection of embeddings (via PCA or t-SNE):\n",
    "\n",
    "| Word  | Coordinates (x, y) |\n",
    "| ----- | ------------------ |\n",
    "| king  | (0.8, 0.6)         |\n",
    "| queen | (0.7, 0.5)         |\n",
    "| man   | (0.3, 0.9)         |\n",
    "| woman | (0.2, 0.8)         |\n",
    "\n",
    "The distances show semantic closeness â†’ â€œkingâ€“queenâ€ and â€œmanâ€“womanâ€ form parallel relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ **13. Summary Table**\n",
    "\n",
    "| Aspect          | Word2Vec                    |\n",
    "| --------------- | --------------------------- |\n",
    "| Type            | Neural embedding            |\n",
    "| Models          | CBOW, Skip-Gram             |\n",
    "| Captures        | Context + Meaning           |\n",
    "| Output          | Dense word vectors          |\n",
    "| Frameworks      | Gensim, TensorFlow, PyTorch |\n",
    "| Example Analogy | King - Man + Woman = Queen  |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to create this as a **PDF version with diagrams and visual representation (CBOW vs Skip-Gram illustration, vector space chart, etc.)** for download?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b99e9a9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
