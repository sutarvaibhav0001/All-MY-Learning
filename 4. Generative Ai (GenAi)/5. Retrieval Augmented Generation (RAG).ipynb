{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeee4052",
   "metadata": {},
   "source": [
    "# üî∑ Real-World Use Case (RAG)\n",
    "\n",
    "Example flow:\n",
    "\n",
    "```\n",
    "User Question\n",
    "     ‚Üì\n",
    "Convert to Embedding\n",
    "     ‚Üì\n",
    "Vector DB Search (FAISS / Chromadb)\n",
    "     ‚Üì\n",
    "Retrieve Similar Docs\n",
    "     ‚Üì\n",
    "Send to LLM\n",
    "     ‚Üì\n",
    "Generate Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056ef45",
   "metadata": {},
   "source": [
    "## üîé Working of RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** is a technique that combines:\n",
    "\n",
    "* üìö **Information Retrieval** (searching relevant documents)\n",
    "* ü§ñ **Text Generation** (LLM generates final answer)\n",
    "\n",
    "It was introduced in the paper\n",
    "**Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks** by researchers at **Facebook AI Research**.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Why RAG?\n",
    "\n",
    "LLMs (like GPT)\n",
    "‚ùå Don‚Äôt have real-time knowledge\n",
    "‚ùå May hallucinate\n",
    "‚ùå Can't access private company data\n",
    "\n",
    "RAG solves this by:\n",
    "\n",
    "> ‚ÄúRetrieving relevant external data first, then generating answer using it.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "# ‚öôÔ∏è Architecture of RAG\n",
    "\n",
    "```\n",
    "User Question\n",
    "      ‚Üì\n",
    "Convert to Embedding\n",
    "      ‚Üì\n",
    "Vector Database Search\n",
    "      ‚Üì\n",
    "Retrieve Top-K Documents\n",
    "      ‚Üì\n",
    "Pass Context + Question to LLM\n",
    "      ‚Üì\n",
    "Final Generated Answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ü™ú Step-by-Step Working (With Example)\n",
    "\n",
    "### üéØ Example Question:\n",
    "\n",
    "> ‚ÄúWhat is the leave policy for maternity leave in our company?‚Äù\n",
    "\n",
    "Assume this info exists inside company PDF documents.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 1: Document Preparation (Offline Step)\n",
    "\n",
    "1. Load documents (PDF, text, etc.)\n",
    "\n",
    "2. Split into chunks\n",
    "\n",
    "3. Convert chunks into embeddings using:\n",
    "\n",
    "   * **OpenAI** embeddings\n",
    "   * **Hugging Face** models\n",
    "\n",
    "4. Store embeddings in Vector DB:\n",
    "\n",
    "   * **FAISS**\n",
    "   * **Chroma**\n",
    "\n",
    "üëâ This step is done only once.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 2: User Asks Question\n",
    "\n",
    "User asks:\n",
    "\n",
    "> ‚ÄúWhat is maternity leave duration?‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 3: Convert Question to Embedding\n",
    "\n",
    "The question is converted into a vector (numerical representation).\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "[0.234, -0.876, 0.455, ...]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 4: Similarity Search\n",
    "\n",
    "Vector DB finds the most similar document chunks using:\n",
    "\n",
    "* Cosine Similarity\n",
    "* Dot Product\n",
    "\n",
    "It retrieves top-k relevant chunks.\n",
    "\n",
    "Example retrieved text:\n",
    "\n",
    "```\n",
    "\"Female employees are entitled to 6 months of paid maternity leave.\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 5: Send Context + Question to LLM\n",
    "\n",
    "Prompt given to LLM:\n",
    "\n",
    "```\n",
    "Context:\n",
    "Female employees are entitled to 6 months of paid maternity leave.\n",
    "\n",
    "Question:\n",
    "What is maternity leave duration?\n",
    "\n",
    "Answer:\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 6: LLM Generates Final Answer\n",
    "\n",
    "Output:\n",
    "\n",
    "> \"The maternity leave duration is 6 months with full pay.\"\n",
    "\n",
    "‚úÖ Accurate\n",
    "‚úÖ Based on company document\n",
    "‚úÖ No hallucination\n",
    "\n",
    "---\n",
    "\n",
    "# üèóÔ∏è Full Pipeline Example (Technical View)\n",
    "\n",
    "```python\n",
    "# 1. Load Documents\n",
    "docs = load_documents()\n",
    "\n",
    "# 2. Create Embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 3. Store in FAISS\n",
    "vector_db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# 4. Retrieve\n",
    "retriever = vector_db.as_retriever()\n",
    "relevant_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# 5. Send to LLM\n",
    "response = llm(context + query)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üìä Simple Real-Life Analogy\n",
    "\n",
    "RAG = üìö Open Book Exam\n",
    "\n",
    "* LLM alone ‚Üí Closed book exam (memory only)\n",
    "* RAG ‚Üí Open book exam (search + answer)\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Advantages of RAG\n",
    "\n",
    "‚úî Reduces hallucination\n",
    "‚úî Uses private data\n",
    "‚úî Real-time knowledge\n",
    "‚úî More accurate\n",
    "‚úî No need to retrain model\n",
    "\n",
    "---\n",
    "\n",
    "# ‚ö†Ô∏è Limitations\n",
    "\n",
    "‚ùå Retrieval quality affects answer\n",
    "‚ùå Large DB = slower search\n",
    "‚ùå Chunking strategy matters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002c67e8",
   "metadata": {},
   "source": [
    "Here are the **üî• Most Asked RAG Interview Questions** with **2-minute crisp answers** (perfect for interviews).\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ What is RAG?\n",
    "\n",
    "**Answer (2 min):**\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) is a technique that combines a **retriever** and a **generator**.\n",
    "First, documents are converted into embeddings and stored in a vector database like **FAISS**.\n",
    "\n",
    "When a user asks a question:\n",
    "\n",
    "1. The query is converted into an embedding.\n",
    "2. Similar documents are retrieved using similarity search.\n",
    "3. Retrieved documents are added to the prompt.\n",
    "4. The LLM generates a grounded answer.\n",
    "\n",
    "It reduces hallucination and allows LLMs to use private or real-time data without retraining.\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Why is RAG needed?\n",
    "\n",
    "**Answer (2 min):**\n",
    "\n",
    "LLMs have:\n",
    "\n",
    "* Fixed training knowledge\n",
    "* No access to private data\n",
    "* Risk of hallucination\n",
    "\n",
    "RAG solves this by retrieving relevant external information before generating the answer.\n",
    "This makes responses:\n",
    "\n",
    "* More accurate\n",
    "* Up-to-date\n",
    "* Context-aware\n",
    "\n",
    "Instead of memorizing everything, the model searches and then answers ‚Äî like an open-book exam.\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ How is RAG different from Fine-Tuning?\n",
    "\n",
    "**Answer (2 min):**\n",
    "\n",
    "RAG retrieves external knowledge dynamically, while fine-tuning updates model weights.\n",
    "\n",
    "| RAG                        | Fine-Tuning                    |\n",
    "| -------------------------- | ------------------------------ |\n",
    "| Uses external documents    | Changes model parameters       |\n",
    "| Cheap & fast               | Expensive                      |\n",
    "| Real-time updates          | Requires retraining            |\n",
    "| Good for knowledge updates | Good for behavior/style change |\n",
    "\n",
    "Use RAG when knowledge changes frequently.\n",
    "Use fine-tuning when you want the model to behave differently.\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Explain RAG Architecture.\n",
    "\n",
    "**Answer (2 min):**\n",
    "\n",
    "RAG has two main components:\n",
    "\n",
    "### 1. Retriever\n",
    "\n",
    "* Converts documents into embeddings\n",
    "* Stores them in a vector database (e.g., FAISS)\n",
    "* Retrieves top-k relevant chunks using cosine similarity\n",
    "\n",
    "### 2. Generator\n",
    "\n",
    "* Takes retrieved context + question\n",
    "* Generates final answer using LLM\n",
    "\n",
    "The concept was introduced in\n",
    "**Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks**.\n",
    "\n",
    "Pipeline:\n",
    "User Query ‚Üí Embedding ‚Üí Vector Search ‚Üí Retrieve Docs ‚Üí LLM ‚Üí Answer\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ What is the role of embeddings in RAG?\n",
    "\n",
    "**Answer (2 min):**\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning.\n",
    "\n",
    "This allows:\n",
    "\n",
    "* Similar meaning texts to be close in vector space\n",
    "* Fast similarity search\n",
    "\n",
    "Without embeddings, semantic search would not be possible.\n",
    "\n",
    "Example:\n",
    "‚ÄúRefund policy‚Äù and ‚ÄúReturn rules‚Äù will have similar embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ What is Top-K Retrieval?\n",
    "\n",
    "**Answer (2 min):**\n",
    "\n",
    "Top-K retrieval means fetching the K most similar document chunks to the query.\n",
    "\n",
    "Example:\n",
    "\n",
    "* K = 3 ‚Üí retrieve top 3 most relevant chunks\n",
    "\n",
    "Choosing K:\n",
    "\n",
    "* Small K ‚Üí faster but less context\n",
    "* Large K ‚Üí more context but risk of noise\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ What is Chunking and Why is it Important?\n",
    "\n",
    "**Answer (2 min):**\n",
    "\n",
    "Chunking means splitting large documents into smaller pieces before embedding.\n",
    "\n",
    "Why important?\n",
    "\n",
    "* LLM has token limits\n",
    "* Smaller chunks improve retrieval accuracy\n",
    "* Large chunks reduce precision\n",
    "\n",
    "Good chunking improves RAG performance significantly.\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ What is Hybrid Search?\n",
    "\n",
    "**Answer (2 min):**\n",
    "\n",
    "Hybrid search combines:\n",
    "\n",
    "* Dense retrieval (embeddings)\n",
    "* Sparse retrieval (BM25 keyword search)\n",
    "\n",
    "Dense search understands meaning.\n",
    "Sparse search matches keywords.\n",
    "\n",
    "Combining both improves recall and accuracy in production systems.\n",
    "\n",
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ How Does RAG Reduce Hallucination?\n",
    "\n",
    "**Answer (2 min):**\n",
    "\n",
    "RAG reduces hallucination by grounding the model‚Äôs answer in retrieved documents.\n",
    "\n",
    "Instead of generating from memory:\n",
    "\n",
    "* It uses actual retrieved evidence\n",
    "* Limits answer to provided context\n",
    "\n",
    "However, if retrieval fails, hallucination can still happen.\n",
    "\n",
    "---\n",
    "\n",
    "# üîü How Do You Evaluate a RAG System?\n",
    "\n",
    "**Answer (2 min):**\n",
    "\n",
    "RAG evaluation has two parts:\n",
    "\n",
    "### Retrieval Evaluation:\n",
    "\n",
    "* Precision@K\n",
    "* Recall@K\n",
    "* MRR (Mean Reciprocal Rank)\n",
    "\n",
    "### Generation Evaluation:\n",
    "\n",
    "* Answer correctness\n",
    "* Faithfulness to context\n",
    "* Human evaluation\n",
    "\n",
    "Good retrieval + good generation = strong RAG system.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0385ce34",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
